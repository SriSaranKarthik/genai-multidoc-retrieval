{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9625ab2-71b6-4fd0-904e-42df80d3215f",
   "metadata": {
    "height": 46,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3221a474-5817-4db2-af46-e029042a75a5",
   "metadata": {
    "height": 46,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bff58c52",
   "metadata": {
    "height": 63
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18771e69",
   "metadata": {},
   "source": [
    "### Download 11 ICLR papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60d01d2c-547f-4054-b0fe-ed9b1a9cc3b5",
   "metadata": {
    "height": 471,
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
    "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
    "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
    "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
    "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
    "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
    "    \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"loftq.pdf\",\n",
    "    \"swebench.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "    \"zipformer.pdf\",\n",
    "    \"values.pdf\",\n",
    "    \"finetune_fair_diffusion.pdf\",\n",
    "    \"knowledge_card.pdf\",\n",
    "    \"metra.pdf\",\n",
    "    \"vr_mcl.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea5ee34d-02ac-4537-ae20-7ef6c5767172",
   "metadata": {
    "height": 148,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: loftq.pdf\n",
      "Getting tools for paper: swebench.pdf\n",
      "Getting tools for paper: selfrag.pdf\n",
      "Getting tools for paper: zipformer.pdf\n",
      "Getting tools for paper: values.pdf\n",
      "Getting tools for paper: finetune_fair_diffusion.pdf\n",
      "Getting tools for paper: knowledge_card.pdf\n",
      "Getting tools for paper: metra.pdf\n",
      "Getting tools for paper: vr_mcl.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20154923-873e-4941-9a3a-4926ab5f9b8c",
   "metadata": {
    "height": 29,
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "671582f9-70d7-4a8f-b813-58b2a068ca72",
   "metadata": {
    "height": 148,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3929882-e9dc-46ca-b495-53e3ed60340e",
   "metadata": {
    "height": 29,
    "tags": []
   },
   "outputs": [],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba9cfecd-fe14-4da8-b9ba-b3d485d98a03",
   "metadata": {
    "height": 63,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = obj_retriever.retrieve(\n",
    "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c270ffbf-69c7-48ea-a028-9ba25221cde5",
   "metadata": {
    "height": 29,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Useful for summarization questions related to swebench', name='summary_tool_swebench', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[2].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cc0a0b6-9858-4348-9ae0-1cd4160f3fb7",
   "metadata": {
    "height": 250,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=obj_retriever,\n",
    "    llm=llm, \n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries over a set of given papers.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a250cf1a-e011-4994-bcca-4e0294f20864",
   "metadata": {
    "height": 97,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in MetaGPT includes HumanEval, MBPP, and the SoftwareDev dataset, which consists of 70 diverse software development tasks covering various scopes such as mini-games, image processing algorithms, and data visualization.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_swebench with args: {\"input\": \"evaluation dataset used in SWE-Bench\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in SWE-Bench consists of task instances collected from public repositories with licenses that permit software usage. It includes task instructions, issue text, retrieved files and documentation, an example patch file, and a prompt for generating the patch file. The dataset is designed to be challenging and realistic for evaluating language models in the context of software engineering tasks. Task instances are validated through execution-based verification to ensure usability and non-triviality of the solutions. The dataset is constructed by scraping pull requests from the top 100 packages of the top 5,000 most downloaded PyPI libraries and is validated to ensure that solutions and tests can be applied, installed, and run successfully on top of the codebase.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in MetaGPT includes HumanEval, MBPP, and the SoftwareDev dataset, which consists of 70 diverse software development tasks covering various scopes such as mini-games, image processing algorithms, and data visualization.\n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench consists of task instances collected from public repositories with licenses that permit software usage. It includes task instructions, issue text, retrieved files and documentation, an example patch file, and a prompt for generating the patch file. The dataset is designed to be challenging and realistic for evaluating language models in the context of software engineering tasks. Task instances are validated through execution-based verification to ensure usability and non-triviality of the solutions. The dataset is constructed by scraping pull requests from the top 100 packages of the top 5,000 most downloaded PyPI libraries and is validated to ensure that solutions and tests can be applied, installed, and run successfully on top of the codebase.\n",
      "assistant: The evaluation dataset used in MetaGPT includes HumanEval, MBPP, and the SoftwareDev dataset, which consists of 70 diverse software development tasks covering various scopes such as mini-games, image processing algorithms, and data visualization.\n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench consists of task instances collected from public repositories with licenses that permit software usage. It includes task instructions, issue text, retrieved files and documentation, an example patch file, and a prompt for generating the patch file. The dataset is designed to be challenging and realistic for evaluating language models in the context of software engineering tasks. Task instances are validated through execution-based verification to ensure usability and non-triviality of the solutions. The dataset is constructed by scraping pull requests from the top 100 packages of the top 5,000 most downloaded PyPI libraries and is validated to ensure that solutions and tests can be applied, installed, and run successfully on top of the codebase.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8084c8cb-98ed-4835-aaa4-5b0c7254be6d",
   "metadata": {
    "height": 80,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ). Analyze the approach in each paper first. \n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"Approach in LongLoRA\"}\n",
      "=== Function Output ===\n",
      "The approach in LongLoRA involves efficiently extending the context length of large language models (LLMs) using shifted sparse attention (S2-Attn) during fine-tuning. This method allows for training models with longer context sizes while saving on computational costs. LongLoRA combines this with a parameter-efficient fine-tuning regime, emphasizing the importance of trainable embedding and normalization layers for successful context extension. The approach retains the original attention architecture during inference, making it compatible with existing optimization techniques and infrastructure.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_loftq with args: {\"input\": \"Approach in LoftQ\"}\n",
      "=== Function Output ===\n",
      "The approach in LoftQ involves integrating quantization and low-rank approximation techniques to approximate the original high-precision pre-trained weights. This method aims to provide an advantageous initialization for subsequent LoRA fine-tuning, alternating between quantization and Singular Value Decomposition (SVD) to optimize the quantized backbone and low-rank adapters. By freezing the integer weight matrix during LoRA fine-tuning and optimizing only the low-rank adapters, LoftQ reduces training costs and enhances generalization in downstream tasks. Additionally, LoftQ focuses on compressing large language models through quantization methods like low-rank approximation and sparse approximation to improve efficiency while maintaining performance, measuring the compression ratio and GPU memory cost during training to demonstrate efficiency gains.\n",
      "=== LLM Response ===\n",
      "The approach in LongLoRA focuses on extending the context length of large language models (LLMs) efficiently using shifted sparse attention (S2-Attn) during fine-tuning. This method enables training models with longer context sizes while saving on computational costs. LongLoRA combines this with a parameter-efficient fine-tuning regime, emphasizing the importance of trainable embedding and normalization layers for successful context extension. The approach retains the original attention architecture during inference, making it compatible with existing optimization techniques and infrastructure.\n",
      "\n",
      "On the other hand, LoftQ's approach involves integrating quantization and low-rank approximation techniques to approximate the original high-precision pre-trained weights. The goal is to provide a beneficial initialization for subsequent LoRA fine-tuning by alternating between quantization and Singular Value Decomposition (SVD) to optimize the quantized backbone and low-rank adapters. LoftQ freezes the integer weight matrix during LoRA fine-tuning and optimizes only the low-rank adapters, reducing training costs and enhancing generalization in downstream tasks. Additionally, LoftQ focuses on compressing large language models through quantization methods like low-rank approximation and sparse approximation to improve efficiency while maintaining performance, measuring the compression ratio and GPU memory cost during training to demonstrate efficiency gains.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
    "    \"Analyze the approach in each paper first. \"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
